{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ESSE_TUTORIAL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_3WbBNZJ8w4"
      },
      "source": [
        "# **Deep Learning embedded into Raspberry PI 3 using Quantized Pytorch Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_4Ghw0IiSRI"
      },
      "source": [
        "Neste tutorial iremos abordar as 3 diferentes técnicas de compressão de modelos utilizando diversos recursos que o framework Pytorch nos oferece. Iremos avaliar o ganho de perfomance em uma plataforma embarcada chamada Raspberry PI 3 e iremos estruturar como dar deploy desse modelo.\n",
        "\n",
        "### **Disclaimer:**\n",
        "Este tutorial se utiliza de diversos tutoriais online para sua criação. Abaixo há todos os links que os levam até eles.\n",
        "\n",
        "[Pytorch Quantize Recipe](https://pytorch.org/docs/stable/quantization.html#quantization-workflows)\n",
        "\n",
        "[Pytorch Quantization Docs](https://pytorch.org/docs/stable/quantization.html)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOdGoANzqG_0"
      },
      "source": [
        "## Relembrando \n",
        "\n",
        "Um neurônio é modelado como um produto interno: \n",
        "\n",
        "$y(\\textbf{X}) = \\beta^\\top \\textbf{X} = \\sum_i \\textbf{w}_i\\textbf{x}_i$\n",
        "\n",
        "obs: o bias está implícito no vetor de pesos.\n",
        "\n",
        "Dessa forma, como podemos treiná-lo?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzKJxNDSIwzl"
      },
      "source": [
        "# Treinando o modelo\n",
        "\n",
        "A função que atualiza os pesos pode ser calculada como:<br>\n",
        "<center>$\\begin{equation}\\Delta w_{i} =  - \\eta.\\nabla_{w}^E\\end{equation}$</center> \n",
        "\n",
        "Onde $\\nabla_{w}^E$ é o operador de gradiente calculado sobre a funação de erro em função dos pesos. Assumindo que cada peso é linearmente independente, podemos reescrever esta função como:<br>\n",
        "Sendo $\\textbf{w}$ um vetor de pesos e  $\\nabla_{\\textbf{w}}^E$ um vetor de derivadas parciais da função de erro em relação aos pesos, então:\n",
        "\n",
        "$\\begin{equation}\n",
        "  \\textbf{w} = \\begin{pmatrix} w_{1} \\\\ w_{2} \\\\ \\vdots \\\\ w_{n} \\end{pmatrix} ; \\nabla_{\\textbf{w}}^E = \\begin{pmatrix} \\frac{\\partial E}{\\partial w_{1}} \\\\ \\frac{\\partial E}{\\partial w_{2}}\\\\ \\vdots \\\\ \\frac{\\partial E}{\\partial w_{n}}\\end{pmatrix} \\end{equation}\n",
        "  $\n",
        "\n",
        "  Portanto $\\textbf{w}_{t+1} = \\textbf{w}_{t} - \\eta.\\nabla_{\\textbf{w}}^E$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3q7tl3nxyJ5l"
      },
      "source": [
        "# **Convolutional Neural Networks**\n",
        "\n",
        "---\n",
        "\n",
        "Convolução em imagens é o processo de aplicar um filtro a uma representação da image. Em redes neurais, essas aplicações são compostas em diversas camadas onde diversos outros filtros são aplicados às representações anteriores da imagem até chegar a etapa de classificação. Essa parte da arquitetura é chamada de extrator de características.\n",
        "\n",
        "A etapa de classificação, geralmente, é composta por uma rede MLP ou fully connected.\n",
        "\n",
        "![conv 1](https://www.researchgate.net/publication/331540139/figure/fig4/AS:733273504354306@1551837435967/The-overall-architecture-of-the-Convolutional-Neural-Network-CNN-includes-an-input.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TArvE3mHqCE"
      },
      "source": [
        "#pip install torch==1.5.0\n",
        "#pip install torchvision==0.5.0\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rgnSCPpL43c",
        "outputId": "5eea35e0-d848-4519-f7d8-aadfb9c99fe6"
      },
      "source": [
        "#@title **Importando dependencias que serao utilizadas neste tutorial**\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "import time\n",
        "import sys\n",
        "import torch.quantization\n",
        "from torch.utils.data import random_split\n",
        "from torch.quantization import QuantStub, DeQuantStub\n",
        "import copy\n",
        "# # Setup warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\n",
        "    action='ignore',\n",
        "    category=DeprecationWarning,\n",
        "    module=r'.*'\n",
        ")\n",
        "warnings.filterwarnings(\n",
        "    action='default',\n",
        "    module=r'torch.quantization'\n",
        ")\n",
        "\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown Para conseguirmos reproduzir aos experimentos de maneira deterministica,\n",
        "#@markdown precisamos configurar uma seed -->```torch.manual_seed(191009)```  \n",
        "# Specify random seed for repeatable results\n",
        "torch.manual_seed(191009)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f35311c4cb0>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Px2cHeB9HlDK"
      },
      "source": [
        ""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3x2R5bNq7cl",
        "outputId": "a886c687-7141-4fc8-ac83-94a7d9216b26"
      },
      "source": [
        "#@title Loading the dataset the same way as before, but now using Normalization\n",
        "transform=transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    \n",
        "])\n",
        "\n",
        "dataset_train = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "dataset_test = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "test, val = random_split(dataset_test, lengths = (5000,5000))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cyG-TXTq-Km"
      },
      "source": [
        "batch_size = 2500\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "train_loader = DataLoader(dataset=dataset_train, shuffle=True, batch_size=batch_size)\n",
        "test_loader = DataLoader(dataset=test, shuffle=False, batch_size=batch_size)\n",
        "val_loader = DataLoader(dataset=val, shuffle=False, batch_size=batch_size)\n",
        "\n",
        "dataloaders = {'train': train_loader, 'val':val_loader, 'test' : test_loader }\n",
        "\n",
        "dataset_sizes = {'train' : len(train_loader.dataset), 'test' : len(test_loader.dataset), 'val': len(val_loader.dataset)}\n",
        "\n",
        "device =  torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcaoDvPb7YQr"
      },
      "source": [
        "class ConvBNReLU(nn.Sequential):\n",
        "    def __init__(self, in_planes, out_planes, padding, kernel_size=3, stride=1, groups=1):\n",
        "        #padding = (kernel_size - 1) // 2\n",
        "        super(ConvBNReLU, self).__init__(\n",
        "            nn.Conv2d(in_planes, out_planes, kernel_size, stride, padding, groups=groups, bias=False),\n",
        "            nn.BatchNorm2d(out_planes, momentum=0.1),\n",
        "            # Replace with ReLU\n",
        "            nn.ReLU(inplace=False)\n",
        "        )\n",
        "\n",
        "class CNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNN,self).__init__()\n",
        "    \n",
        "    #input_channel, output_channel, feature_dimension(kernel_size), stride, padding\n",
        "    self.feats = nn.Sequential(\n",
        "        \n",
        "        ConvBNReLU(3, 20, kernel_size = 3, stride = 1, padding=0),#30x30x20\n",
        "        nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1),#15x15x20\n",
        "\n",
        "        \n",
        "        \n",
        "\n",
        "        ConvBNReLU(20, 256, kernel_size = 3, stride = 1, padding = 1),#15x15x256\n",
        "        nn.MaxPool2d(kernel_size = 3, stride = 2),#7x7x256\n",
        "      \n",
        "        nn.MaxPool2d(kernel_size = 3, stride = 2 )#3x3x256\n",
        "    )\n",
        "    self.fc = nn.Sequential(\n",
        "        nn.Dropout(0.4),\n",
        "        nn.Linear(3*3*256,10),\n",
        "        # nn.ReLU(True),\n",
        "        # nn.Dropout(0.5),\n",
        "        # nn.Linear(768, 10),\n",
        "        #nn.LogSoftmax(1)\n",
        "    )\n",
        "    \n",
        "    self.quant = QuantStub()\n",
        "    \n",
        "    self.dequant = DeQuantStub()\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.quant(x)\n",
        "    x = self.feats(x) # CNN\n",
        "\n",
        "    x = x.reshape(-1,3*3*256) # Lineariza \n",
        "\n",
        "    x = self.fc(x) #Classifica\n",
        "    x = self.dequant(x)\n",
        "    return x\n",
        "\n",
        "  def fuse_model(self):\n",
        "    for m in self.modules():\n",
        "        if type(m) == ConvBNReLU:\n",
        "            torch.quantization.fuse_modules(m, ['0', '1', '2'], inplace=True)\n",
        "        "
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pt8Umows1ETF"
      },
      "source": [
        "def train_model(model, criterion, optimizer, dataloaders, device='cpu', num_epochs=25):\n",
        "    since = time.time()\n",
        "    \n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "            # if phase == 'train':\n",
        "            #     scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_HwgRANL-L5"
      },
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self, name, fmt=':f'):\n",
        "        self.name = name\n",
        "        self.fmt = fmt\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "    def __str__(self):\n",
        "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
        "        return fmtstr.format(**self.__dict__)\n",
        "\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
        "    with torch.no_grad():\n",
        "        maxk = max(topk)\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        _, pred = output.topk(maxk, 1, True, True)\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "        res = []\n",
        "        for k in topk:\n",
        "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "            res.append(correct_k.mul_(100.0 / batch_size))\n",
        "        return res\n",
        "\n",
        "\n",
        "def evaluate(model, criterion, data_loader, neval_batches):\n",
        "    model.eval()\n",
        "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
        "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
        "    cnt = 0\n",
        "    with torch.no_grad():\n",
        "        for image, target in data_loader:\n",
        "            output = model(image)\n",
        "            loss = criterion(output, target)\n",
        "            cnt += 1\n",
        "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
        "            print('.', end = '')\n",
        "            top1.update(acc1[0], image.size(0))\n",
        "            top5.update(acc5[0], image.size(0))\n",
        "            if cnt >= neval_batches:\n",
        "                 return top1, top5\n",
        "\n",
        "    return top1, top5\n",
        "\n",
        "def load_model(model_file):\n",
        "    model = CNN()\n",
        "    state_dict = torch.load(model_file)\n",
        "    model.load_state_dict(state_dict)\n",
        "    model.to('cpu')\n",
        "    return model\n",
        "\n",
        "def print_size_of_model(model):\n",
        "    torch.save(model.state_dict(), \"temp.p\")\n",
        "    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
        "    os.remove('temp.p')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLN1Ef6W0psG",
        "outputId": "f6c603ba-c033-4f4b-cc37-c4769b885fe3"
      },
      "source": [
        "model = CNN()\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "train_model(model.to(device), criterion, optimizer, dataloaders, device=device, num_epochs=3)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/2\n",
            "----------\n",
            "train Loss: 2.6436 Acc: 0.1295\n",
            "val Loss: 2.1235 Acc: 0.2598\n",
            "\n",
            "Epoch 1/2\n",
            "----------\n",
            "train Loss: 2.2474 Acc: 0.2123\n",
            "val Loss: 1.8728 Acc: 0.3530\n",
            "\n",
            "Epoch 2/2\n",
            "----------\n",
            "train Loss: 2.0625 Acc: 0.2665\n",
            "val Loss: 1.7545 Acc: 0.3946\n",
            "\n",
            "Training complete in 1m 5s\n",
            "Best val Acc: 0.394600\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNN(\n",
              "  (feats): Sequential(\n",
              "    (0): ConvBNReLU(\n",
              "      (0): Conv2d(3, 20, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (1): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    (2): ConvBNReLU(\n",
              "      (0): Conv2d(20, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (4): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (fc): Sequential(\n",
              "    (0): Dropout(p=0.4, inplace=False)\n",
              "    (1): Linear(in_features=2304, out_features=10, bias=True)\n",
              "  )\n",
              "  (quant): QuantStub()\n",
              "  (dequant): DeQuantStub()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRYYioaL7PKx"
      },
      "source": [
        "torch.save(model.state_dict(), 'model_cnn_basic.pth')"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYkKbR89fDdi"
      },
      "source": [
        "torch.jit.save(torch.jit.script(model.to('cpu')), 'model_cnn_basic_scripted.pth')"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoO8QMCvMDdU"
      },
      "source": [
        "saved_model_dir = './'\n",
        "float_model_file = 'model_cnn_basic.pth'\n",
        "scripted_float_model_file = 'model_cnn_basic_quantization_scripted.pth'\n",
        "scripted_quantized_model_file = 'model_cnn_basic_quantization_scripted_quantized.pth'\n",
        "\n",
        "train_batch_size = 30\n",
        "eval_batch_size = 50\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "float_model = load_model(saved_model_dir + float_model_file).to('cpu')\n",
        "\n",
        "\n",
        "\n",
        "float_model.eval()\n",
        "\n",
        "# Fuses modules\n",
        "float_model.fuse_model()\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsFr_2IzMFeg",
        "outputId": "50239551-f19b-44ef-c9fd-e358b54ed851"
      },
      "source": [
        "num_eval_batches = 1000\n",
        "\n",
        "print(\"Size of baseline model\")\n",
        "print_size_of_model(float_model)\n",
        "\n",
        "top1, top5 = evaluate(float_model, criterion, dataloaders['val'], neval_batches=num_eval_batches)\n",
        "print('\\nEvaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg))\n",
        "torch.jit.save(torch.jit.script(float_model), saved_model_dir + scripted_float_model_file)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of baseline model\n",
            "Size (MB): 0.281291\n",
            "..\n",
            "Evaluation accuracy on 50000 images, 39.46\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvQy3dFoMI02",
        "outputId": "6c5315d7-9de0-4097-e7b2-3967c1a79a65"
      },
      "source": [
        "num_calibration_batches = 32\n",
        "\n",
        "myModel = load_model(saved_model_dir + float_model_file).to('cpu')\n",
        "myModel.eval()\n",
        "\n",
        "# Fuse Conv, bn and relu\n",
        "myModel.fuse_model()\n",
        "\n",
        "# Specify quantization configuration\n",
        "# Start with simple min/max range estimation and per-tensor quantization of weights\n",
        "myModel.qconfig = torch.quantization.default_qconfig\n",
        "print(myModel.qconfig)\n",
        "backend = \"qnnpack\"\n",
        "\n",
        "torch.backends.quantized.engine = backend\n",
        "torch.quantization.prepare(myModel, inplace=True)\n",
        "\n",
        "# Calibrate first\n",
        "print('Post Training Quantization Prepare: Inserting Observers')\n",
        "#print('\\n Inverted Residual Block:After observer insertion \\n\\n', myModel.features[1].conv)\n",
        "\n",
        "# Calibrate with the training set\n",
        "evaluate(myModel, criterion, dataloaders['val'], neval_batches=num_calibration_batches)\n",
        "print('Post Training Quantization: Calibration done')\n",
        "\n",
        "# Convert to quantized model\n",
        "torch.quantization.convert(myModel, inplace=True)\n",
        "print('Post Training Quantization: Convert done')\n",
        "#print('\\n Inverted Residual Block: After fusion and quantization, note fused modules: \\n\\n',myModel.features[1].conv)\n",
        "\n",
        "print(\"Size of model after quantization\")\n",
        "print_size_of_model(myModel)\n",
        "\n",
        "top1, top5 = evaluate(myModel, criterion, dataloaders['val'], neval_batches=num_eval_batches)\n",
        "print('\\nEvaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QConfig(activation=functools.partial(<class 'torch.quantization.observer.MinMaxObserver'>, reduce_range=True), weight=functools.partial(<class 'torch.quantization.observer.MinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric))\n",
            "Post Training Quantization Prepare: Inserting Observers\n",
            "..Post Training Quantization: Calibration done\n",
            "Post Training Quantization: Convert done\n",
            "Size of model after quantization\n",
            "Size (MB): 0.07332\n",
            "..\n",
            "Evaluation accuracy on 50000 images, 38.94\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVLeK-uYMKfG",
        "outputId": "8f224a90-1b69-49ad-f4ea-eebf33d00345"
      },
      "source": [
        "per_channel_quantized_model = load_model(saved_model_dir + float_model_file)\n",
        "per_channel_quantized_model.eval()\n",
        "per_channel_quantized_model.fuse_model()\n",
        "per_channel_quantized_model.qconfig = torch.quantization.get_default_qconfig('qnnpack')\n",
        "print(per_channel_quantized_model.qconfig)\n",
        "\n",
        "torch.quantization.prepare(per_channel_quantized_model, inplace=True)\n",
        "evaluate(per_channel_quantized_model,criterion, dataloaders['val'], num_calibration_batches)\n",
        "torch.quantization.convert(per_channel_quantized_model, inplace=True)\n",
        "top1, top5 = evaluate(per_channel_quantized_model, criterion, dataloaders['val'], neval_batches=num_eval_batches)\n",
        "print('\\nEvaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg))\n",
        "torch.jit.save(torch.jit.script(per_channel_quantized_model), saved_model_dir + scripted_quantized_model_file)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QConfig(activation=functools.partial(<class 'torch.quantization.observer.HistogramObserver'>, reduce_range=False), weight=functools.partial(<class 'torch.quantization.observer.MinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric))\n",
            "....\n",
            "Evaluation accuracy on 50000 images, 39.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3xQMf6JMMWK"
      },
      "source": [
        "def train_one_epoch(model, criterion, optimizer, data_loader, device, ntrain_batches):\n",
        "    model.train()\n",
        "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
        "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
        "    avgloss = AverageMeter('Loss', '1.5f')\n",
        "\n",
        "    cnt = 0\n",
        "    for image, target in data_loader:\n",
        "        start_time = time.time()\n",
        "        print('.', end = '')\n",
        "        cnt += 1\n",
        "        image, target = image.to(device), target.to(device)\n",
        "        output = model(image)\n",
        "        loss = criterion(output, target)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
        "        top1.update(acc1[0], image.size(0))\n",
        "        top5.update(acc5[0], image.size(0))\n",
        "        avgloss.update(loss, image.size(0))\n",
        "        if cnt >= ntrain_batches:\n",
        "            print('Loss', avgloss.avg)\n",
        "\n",
        "            print('Training: * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'\n",
        "                  .format(top1=top1, top5=top5))\n",
        "            return\n",
        "\n",
        "    print('Full CIFAR10 train set:  * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'\n",
        "          .format(top1=top1, top5=top5))\n",
        "    return"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZku6BJjMObg"
      },
      "source": [
        "qat_model = load_model(saved_model_dir + float_model_file)\n",
        "qat_model.fuse_model()\n",
        "\n",
        "optimizer = torch.optim.SGD(qat_model.parameters(), lr = 0.0001)\n",
        "qat_model.qconfig = torch.quantization.get_default_qat_qconfig('qnnpack')"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeR3gzkAMRs3",
        "outputId": "c6392ef1-e2ca-4bde-b968-159517d0a670"
      },
      "source": [
        "torch.quantization.prepare_qat(qat_model, inplace=True)\n",
        "#print('Inverted Residual Block: After preparation for QAT, note fake-quantization modules \\n',qat_model.features[1].conv)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNN(\n",
              "  (feats): Sequential(\n",
              "    (0): ConvBNReLU(\n",
              "      (0): ConvBnReLU2d(\n",
              "        3, 20, kernel_size=(3, 3), stride=(1, 1), bias=False\n",
              "        (activation_post_process): FakeQuantize(\n",
              "          fake_quant_enabled=True, observer_enabled=True,            scale=None, zero_point=None\n",
              "          (activation_post_process): MovingAverageMinMaxObserver(min_val=None, max_val=None)\n",
              "        )\n",
              "        (weight_fake_quant): FakeQuantize(\n",
              "          fake_quant_enabled=True, observer_enabled=True,            scale=None, zero_point=None\n",
              "          (activation_post_process): MovingAverageMinMaxObserver(min_val=None, max_val=None)\n",
              "        )\n",
              "      )\n",
              "      (1): Identity()\n",
              "      (2): Identity()\n",
              "    )\n",
              "    (1): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    (2): ConvBNReLU(\n",
              "      (0): ConvBnReLU2d(\n",
              "        20, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
              "        (activation_post_process): FakeQuantize(\n",
              "          fake_quant_enabled=True, observer_enabled=True,            scale=None, zero_point=None\n",
              "          (activation_post_process): MovingAverageMinMaxObserver(min_val=None, max_val=None)\n",
              "        )\n",
              "        (weight_fake_quant): FakeQuantize(\n",
              "          fake_quant_enabled=True, observer_enabled=True,            scale=None, zero_point=None\n",
              "          (activation_post_process): MovingAverageMinMaxObserver(min_val=None, max_val=None)\n",
              "        )\n",
              "      )\n",
              "      (1): Identity()\n",
              "      (2): Identity()\n",
              "    )\n",
              "    (3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (4): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (fc): Sequential(\n",
              "    (0): Dropout(p=0.4, inplace=False)\n",
              "    (1): Linear(\n",
              "      in_features=2304, out_features=10, bias=True\n",
              "      (activation_post_process): FakeQuantize(\n",
              "        fake_quant_enabled=True, observer_enabled=True,            scale=None, zero_point=None\n",
              "        (activation_post_process): MovingAverageMinMaxObserver(min_val=None, max_val=None)\n",
              "      )\n",
              "      (weight_fake_quant): FakeQuantize(\n",
              "        fake_quant_enabled=True, observer_enabled=True,            scale=None, zero_point=None\n",
              "        (activation_post_process): MovingAverageMinMaxObserver(min_val=None, max_val=None)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (quant): QuantStub(\n",
              "    (activation_post_process): FakeQuantize(\n",
              "      fake_quant_enabled=True, observer_enabled=True,            scale=None, zero_point=None\n",
              "      (activation_post_process): MovingAverageMinMaxObserver(min_val=None, max_val=None)\n",
              "    )\n",
              "  )\n",
              "  (dequant): DeQuantStub(\n",
              "    (activation_post_process): FakeQuantize(\n",
              "      fake_quant_enabled=True, observer_enabled=True,            scale=None, zero_point=None\n",
              "      (activation_post_process): MovingAverageMinMaxObserver(min_val=None, max_val=None)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W43dNZ_WMVWc",
        "outputId": "0eb3a307-1a2b-4007-d702-c47757fd1bb5"
      },
      "source": [
        "def run_benchmark(model_file, img_loader):\n",
        "    elapsed = 0\n",
        "    model = torch.jit.load(model_file)\n",
        "    model.eval()\n",
        "    num_batches = 1\n",
        "    # Run the scripted model on a few batches of images\n",
        "    for i, (images, target) in enumerate(img_loader):\n",
        "        if i < num_batches:\n",
        "            start = time.time()\n",
        "            output = model(images)\n",
        "            end = time.time()\n",
        "            elapsed = elapsed + (end-start)\n",
        "        else:\n",
        "            break\n",
        "    num_images = images.size()[0] * num_batches\n",
        "\n",
        "    print('Elapsed time: %3.3f ms' % (elapsed/num_images*1000))\n",
        "    return elapsed\n",
        "\n",
        "run_benchmark(saved_model_dir + scripted_float_model_file, dataloaders['val'])\n",
        "\n",
        "run_benchmark(saved_model_dir + scripted_quantized_model_file, dataloaders['val'])"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elapsed time: 1.678 ms\n",
            "Elapsed time: 1.508 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.7700726985931396"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7uT-vu5MThN",
        "outputId": "855b9429-0f47-4abc-a3b1-e775be63c972"
      },
      "source": [
        "num_train_batches = 20\n",
        "\n",
        "# QAT takes time and one needs to train over a few epochs.\n",
        "# Train and check accuracy after each epoch\n",
        "for nepoch in range(8):\n",
        "    train_one_epoch(qat_model, criterion, optimizer, dataloaders['test'], torch.device('cpu'), num_train_batches)\n",
        "    if nepoch > 3:\n",
        "        # Freeze quantizer parameters\n",
        "        qat_model.apply(torch.quantization.disable_observer)\n",
        "    if nepoch > 2:\n",
        "        # Freeze batch norm mean and variance estimates\n",
        "        qat_model.apply(torch.nn.intrinsic.qat.freeze_bn_stats)\n",
        "\n",
        "    # Check the accuracy after each epoch\n",
        "    quantized_model = torch.quantization.convert(qat_model.eval(), inplace=False)\n",
        "    quantized_model.eval()\n",
        "    top1, top5 = evaluate(quantized_model,criterion, dataloaders['val'], neval_batches=num_eval_batches)\n",
        "    print('\\nEpoch %d :Evaluation accuracy on %d images, %2.2f'%(nepoch, num_eval_batches * eval_batch_size, top1.avg))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "..Full CIFAR10 train set:  * Acc@1 28.620 Acc@5 81.420\n",
            "..\n",
            "Epoch 0 :Evaluation accuracy on 50000 images, 39.36\n",
            "..Full CIFAR10 train set:  * Acc@1 29.680 Acc@5 81.000\n",
            "..\n",
            "Epoch 1 :Evaluation accuracy on 50000 images, 39.60\n",
            "..Full CIFAR10 train set:  * Acc@1 28.940 Acc@5 80.800\n",
            "..\n",
            "Epoch 2 :Evaluation accuracy on 50000 images, 39.50\n",
            "..Full CIFAR10 train set:  * Acc@1 28.960 Acc@5 80.080\n",
            "..\n",
            "Epoch 3 :Evaluation accuracy on 50000 images, 39.58\n",
            "..Full CIFAR10 train set:  * Acc@1 29.020 Acc@5 80.840\n",
            "..\n",
            "Epoch 4 :Evaluation accuracy on 50000 images, 39.60\n",
            "..Full CIFAR10 train set:  * Acc@1 29.820 Acc@5 81.460\n",
            "..\n",
            "Epoch 5 :Evaluation accuracy on 50000 images, 39.68\n",
            "..Full CIFAR10 train set:  * Acc@1 30.040 Acc@5 81.980\n",
            "..\n",
            "Epoch 6 :Evaluation accuracy on 50000 images, 39.62\n",
            "..Full CIFAR10 train set:  * Acc@1 28.540 Acc@5 81.860\n",
            "..\n",
            "Epoch 7 :Evaluation accuracy on 50000 images, 39.72\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAuq-w_tpvq1",
        "outputId": "7ed5a50f-fed6-4683-f27f-78f673c0ca4a"
      },
      "source": [
        "torch.quantization.convert(quantized_model, inplace=True)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNN(\n",
              "  (feats): Sequential(\n",
              "    (0): ConvBNReLU(\n",
              "      (0): QuantizedConvReLU2d(3, 20, kernel_size=(3, 3), stride=(1, 1), scale=0.027072325348854065, zero_point=0)\n",
              "      (1): Identity()\n",
              "      (2): Identity()\n",
              "    )\n",
              "    (1): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    (2): ConvBNReLU(\n",
              "      (0): QuantizedConvReLU2d(20, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.03293272480368614, zero_point=0, padding=(1, 1))\n",
              "      (1): Identity()\n",
              "      (2): Identity()\n",
              "    )\n",
              "    (3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (4): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (fc): Sequential(\n",
              "    (0): Dropout(p=0.4, inplace=False)\n",
              "    (1): QuantizedLinear(\n",
              "      in_features=2304, out_features=10, scale=0.0460919588804245, zero_point=146\n",
              "      (_packed_params): LinearPackedParams()\n",
              "    )\n",
              "  )\n",
              "  (quant): Quantize(scale=tensor([0.0187]), zero_point=tensor([114]), dtype=torch.quint8)\n",
              "  (dequant): DeQuantize()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBRK7tg7Kd9D"
      },
      "source": [
        "torch.jit.save(torch.jit.script(quantized_model), 'qat_model.pth')"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jj97_PtGURzA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}