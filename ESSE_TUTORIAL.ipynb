{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ESSE_TUTORIAL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "16c0a395fb394447afdd65a96c8ba980": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e9982d33ff8340ccb7a3d7ef7f9eedef",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f7290961b2d44140ab2fc42d6119f82b",
              "IPY_MODEL_b0614178c349400a93a864605d0dc38b",
              "IPY_MODEL_161bb3f0a75b4412aa271af98b882ea3"
            ]
          }
        },
        "e9982d33ff8340ccb7a3d7ef7f9eedef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f7290961b2d44140ab2fc42d6119f82b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a5057f9c216e48f6ad6346d524bd8b9d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_306d4d1abe5242249620b90a6d1a56b1"
          }
        },
        "b0614178c349400a93a864605d0dc38b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6f15b9db204c4c9d9d428d3416072db1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 170498071,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 170498071,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_100d2b8a4c8f4c91932f21a55d0905d9"
          }
        },
        "161bb3f0a75b4412aa271af98b882ea3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_60965e670f3d4670a620cde8d21fbcc0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170499072/? [00:03&lt;00:00, 57652134.33it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7ceb6052a4974b4fb8c5135a74ee7773"
          }
        },
        "a5057f9c216e48f6ad6346d524bd8b9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "306d4d1abe5242249620b90a6d1a56b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6f15b9db204c4c9d9d428d3416072db1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "100d2b8a4c8f4c91932f21a55d0905d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "60965e670f3d4670a620cde8d21fbcc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7ceb6052a4974b4fb8c5135a74ee7773": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_3WbBNZJ8w4"
      },
      "source": [
        "# **Deep Learning embedded into Raspberry PI 3 using Quantized Pytorch Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_4Ghw0IiSRI"
      },
      "source": [
        "Neste tutorial iremos abordar as 3 diferentes técnicas de compressão de modelos utilizando diversos recursos que o framework Pytorch nos oferece. Iremos avaliar o ganho de perfomance em uma plataforma embarcada chamada Raspberry PI 3 e iremos estruturar como dar deploy desse modelo.\n",
        "\n",
        "### **Disclaimer:**\n",
        "Este tutorial se utiliza de diversos tutoriais online para sua criação. Abaixo há todos os links que os levam até eles.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOdGoANzqG_0"
      },
      "source": [
        "## Relembrando \n",
        "\n",
        "Um neurônio é modelado como um produto interno: \n",
        "\n",
        "$y(\\textbf{X}) = \\beta^\\top \\textbf{X} = \\sum_i \\textbf{w}_i\\textbf{x}_i$\n",
        "\n",
        "obs: o bias está implícito no vetor de pesos.\n",
        "\n",
        "Dessa forma, como podemos treiná-lo?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzKJxNDSIwzl"
      },
      "source": [
        "# Treinando o modelo\n",
        "\n",
        "A função que atualiza os pesos pode ser calculada como:<br>\n",
        "<center>$\\begin{equation}\\Delta w_{i} =  - \\eta.\\nabla_{w}^E\\end{equation}$</center> \n",
        "\n",
        "Onde $\\nabla_{w}^E$ é o operador de gradiente calculado sobre a funação de erro em função dos pesos. Assumindo que cada peso é linearmente independente, podemos reescrever esta função como:<br>\n",
        "Sendo $\\textbf{w}$ um vetor de pesos e  $\\nabla_{\\textbf{w}}^E$ um vetor de derivadas parciais da função de erro em relação aos pesos, então:\n",
        "\n",
        "$\\begin{equation}\n",
        "  \\textbf{w} = \\begin{pmatrix} w_{1} \\\\ w_{2} \\\\ \\vdots \\\\ w_{n} \\end{pmatrix} ; \\nabla_{\\textbf{w}}^E = \\begin{pmatrix} \\frac{\\partial E}{\\partial w_{1}} \\\\ \\frac{\\partial E}{\\partial w_{2}}\\\\ \\vdots \\\\ \\frac{\\partial E}{\\partial w_{n}}\\end{pmatrix} \\end{equation}\n",
        "  $\n",
        "\n",
        "  Portanto $\\textbf{w}_{t+1} = \\textbf{w}_{t} - \\eta.\\nabla_{\\textbf{w}}^E$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3q7tl3nxyJ5l"
      },
      "source": [
        "# **Convolutional Neural Networks**\n",
        "\n",
        "---\n",
        "\n",
        "Convolução em imagens é o processo de aplicar um filtro a uma representação da image. Em redes neurais, essas aplicações são compostas em diversas camadas onde diversos outros filtros são aplicados às representações anteriores da imagem até chegar a etapa de classificação. Essa parte da arquitetura é chamada de extrator de características.\n",
        "\n",
        "A etapa de classificação, geralmente, é composta por uma rede MLP ou fully connected.\n",
        "\n",
        "![conv 1](https://www.researchgate.net/publication/331540139/figure/fig4/AS:733273504354306@1551837435967/The-overall-architecture-of-the-Convolutional-Neural-Network-CNN-includes-an-input.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rgnSCPpL43c",
        "outputId": "adf1ec6c-00a6-428d-89e6-1af770a481bb"
      },
      "source": [
        "#@title **Importando dependencias que serao utilizadas neste tutorial**\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "import time\n",
        "import sys\n",
        "import torch.quantization\n",
        "from torch.utils.data import random_split\n",
        "from torch.quantization import QuantStub, DeQuantStub\n",
        "import copy\n",
        "# # Setup warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\n",
        "    action='ignore',\n",
        "    category=DeprecationWarning,\n",
        "    module=r'.*'\n",
        ")\n",
        "warnings.filterwarnings(\n",
        "    action='default',\n",
        "    module=r'torch.quantization'\n",
        ")\n",
        "\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown Para conseguirmos reproduzir aos experimentos de maneira deterministica,\n",
        "#@markdown precisamos configurar uma seed -->```torch.manual_seed(191009)```  \n",
        "# Specify random seed for repeatable results\n",
        "torch.manual_seed(191009)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f5f92d6d4f0>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "16c0a395fb394447afdd65a96c8ba980",
            "e9982d33ff8340ccb7a3d7ef7f9eedef",
            "f7290961b2d44140ab2fc42d6119f82b",
            "b0614178c349400a93a864605d0dc38b",
            "161bb3f0a75b4412aa271af98b882ea3",
            "a5057f9c216e48f6ad6346d524bd8b9d",
            "306d4d1abe5242249620b90a6d1a56b1",
            "6f15b9db204c4c9d9d428d3416072db1",
            "100d2b8a4c8f4c91932f21a55d0905d9",
            "60965e670f3d4670a620cde8d21fbcc0",
            "7ceb6052a4974b4fb8c5135a74ee7773"
          ]
        },
        "id": "k3x2R5bNq7cl",
        "outputId": "dfcfe6aa-5f41-48df-c080-32b0262cd8a3"
      },
      "source": [
        "#@title Loading the dataset the same way as before, but now using Normalization\n",
        "transform=transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    \n",
        "])\n",
        "\n",
        "dataset_train = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "dataset_test = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "test, val = random_split(dataset_test, lengths = (5000,5000))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "16c0a395fb394447afdd65a96c8ba980",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cyG-TXTq-Km"
      },
      "source": [
        "batch_size = 2500\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "train_loader = DataLoader(dataset=dataset_train, shuffle=True, batch_size=batch_size)\n",
        "test_loader = DataLoader(dataset=test, shuffle=False, batch_size=batch_size)\n",
        "val_loader = DataLoader(dataset=val, shuffle=False, batch_size=batch_size)\n",
        "\n",
        "dataloaders = {'train': train_loader, 'val':val_loader, 'test' : test_loader }\n",
        "\n",
        "dataset_sizes = {'train' : len(train_loader.dataset), 'test' : len(test_loader.dataset), 'val': len(val_loader.dataset)}\n",
        "\n",
        "device =  torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcaoDvPb7YQr"
      },
      "source": [
        "class ConvBNReLU(nn.Sequential):\n",
        "    def __init__(self, in_planes, out_planes, padding, kernel_size=3, stride=1, groups=1):\n",
        "        #padding = (kernel_size - 1) // 2\n",
        "        super(ConvBNReLU, self).__init__(\n",
        "            nn.Conv2d(in_planes, out_planes, kernel_size, stride, padding, groups=groups, bias=False),\n",
        "            nn.BatchNorm2d(out_planes, momentum=0.1),\n",
        "            # Replace with ReLU\n",
        "            nn.ReLU(inplace=False)\n",
        "        )\n",
        "\n",
        "class CNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNN,self).__init__()\n",
        "    \n",
        "    #input_channel, output_channel, feature_dimension(kernel_size), stride, padding\n",
        "    self.feats = nn.Sequential(\n",
        "        \n",
        "\n",
        "        nn.Conv2d(3, 20, kernel_size = 3, stride = 1, bias =False), #30x30x20\n",
        "        nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1),#15x15x20\n",
        "        nn.ReLU(True),\n",
        "        nn.BatchNorm2d(20),\n",
        "        \n",
        "        \n",
        "\n",
        "        ConvBNReLU(20, 256, kernel_size = 3, stride = 1, padding = 1),#15x15x256\n",
        "        nn.MaxPool2d(kernel_size = 3, stride = 2),#7x7x256\n",
        "      \n",
        "        nn.MaxPool2d(kernel_size = 3, stride = 2 )#3x3x256\n",
        "    )\n",
        "    self.fc = nn.Sequential(\n",
        "        nn.Dropout(0.4),\n",
        "        nn.Linear(3*3*256,10),\n",
        "        # nn.ReLU(True),\n",
        "        # nn.Dropout(0.5),\n",
        "        # nn.Linear(768, 10),\n",
        "        #nn.LogSoftmax(1)\n",
        "    )\n",
        "    \n",
        "    self.quant = QuantStub()\n",
        "    \n",
        "    self.dequant = DeQuantStub()\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.quant(x)\n",
        "    x = self.feats(x) # CNN\n",
        "\n",
        "    x = x.reshape(-1,3*3*256) # Lineariza \n",
        "\n",
        "    x = self.fc(x) #Classifica\n",
        "    x = self.dequant(x)\n",
        "    return x\n",
        "\n",
        "  def fuse_model(self):\n",
        "    for m in self.modules():\n",
        "        if type(m) == ConvBNReLU:\n",
        "            torch.quantization.fuse_modules(m, ['0', '1', '2'], inplace=True)\n",
        "        "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pt8Umows1ETF"
      },
      "source": [
        "def train_model(model, criterion, optimizer, dataloaders, device='cpu', num_epochs=25):\n",
        "    since = time.time()\n",
        "    \n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "            # if phase == 'train':\n",
        "            #     scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_HwgRANL-L5"
      },
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self, name, fmt=':f'):\n",
        "        self.name = name\n",
        "        self.fmt = fmt\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "    def __str__(self):\n",
        "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
        "        return fmtstr.format(**self.__dict__)\n",
        "\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
        "    with torch.no_grad():\n",
        "        maxk = max(topk)\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        _, pred = output.topk(maxk, 1, True, True)\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "        res = []\n",
        "        for k in topk:\n",
        "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "            res.append(correct_k.mul_(100.0 / batch_size))\n",
        "        return res\n",
        "\n",
        "\n",
        "def evaluate(model, criterion, data_loader, neval_batches):\n",
        "    model.eval()\n",
        "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
        "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
        "    cnt = 0\n",
        "    with torch.no_grad():\n",
        "        for image, target in data_loader:\n",
        "            output = model(image)\n",
        "            loss = criterion(output, target)\n",
        "            cnt += 1\n",
        "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
        "            print('.', end = '')\n",
        "            top1.update(acc1[0], image.size(0))\n",
        "            top5.update(acc5[0], image.size(0))\n",
        "            if cnt >= neval_batches:\n",
        "                 return top1, top5\n",
        "\n",
        "    return top1, top5\n",
        "\n",
        "def load_model(model_file):\n",
        "    model = CNN()\n",
        "    state_dict = torch.load(model_file)\n",
        "    model.load_state_dict(state_dict)\n",
        "    model.to('cpu')\n",
        "    return model\n",
        "\n",
        "def print_size_of_model(model):\n",
        "    torch.save(model.state_dict(), \"temp.p\")\n",
        "    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
        "    os.remove('temp.p')"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLN1Ef6W0psG",
        "outputId": "dfe757c1-38fc-4651-bf19-6ece6c69ca08"
      },
      "source": [
        "model = CNN()\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "train_model(model.to(device), criterion, optimizer, dataloaders, device=device, num_epochs=3)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/2\n",
            "----------\n",
            "train Loss: 2.5479 Acc: 0.1459\n",
            "val Loss: 2.0821 Acc: 0.2720\n",
            "\n",
            "Epoch 1/2\n",
            "----------\n",
            "train Loss: 2.1963 Acc: 0.2285\n",
            "val Loss: 1.8619 Acc: 0.3522\n",
            "\n",
            "Epoch 2/2\n",
            "----------\n",
            "train Loss: 2.0465 Acc: 0.2770\n",
            "val Loss: 1.7662 Acc: 0.3744\n",
            "\n",
            "Training complete in 1m 18s\n",
            "Best val Acc: 0.374400\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNN(\n",
              "  (feats): Sequential(\n",
              "    (0): Conv2d(3, 20, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
              "    (1): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (4): ConvBNReLU(\n",
              "      (0): Conv2d(20, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (6): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (fc): Sequential(\n",
              "    (0): Dropout(p=0.4, inplace=False)\n",
              "    (1): Linear(in_features=2304, out_features=10, bias=True)\n",
              "  )\n",
              "  (quant): QuantStub()\n",
              "  (dequant): DeQuantStub()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRYYioaL7PKx"
      },
      "source": [
        "torch.save(model.state_dict(), 'model_cnn_basic.pth')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkBy_o_oYjvZ",
        "outputId": "24e6405e-a8df-4563-ab4b-3e411b62594f"
      },
      "source": [
        "model"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNN(\n",
              "  (feats): Sequential(\n",
              "    (0): Conv2d(3, 20, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
              "    (1): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (4): ConvBNReLU(\n",
              "      (0): Conv2d(20, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (6): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (fc): Sequential(\n",
              "    (0): Dropout(p=0.4, inplace=False)\n",
              "    (1): Linear(in_features=2304, out_features=10, bias=True)\n",
              "  )\n",
              "  (quant): QuantStub()\n",
              "  (dequant): DeQuantStub()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoO8QMCvMDdU"
      },
      "source": [
        "saved_model_dir = './'\n",
        "float_model_file = 'model_cnn_basic.pth'\n",
        "scripted_float_model_file = 'model_cnn_basic_quantization_scripted.pth'\n",
        "scripted_quantized_model_file = 'model_cnn_basic_quantization_scripted_quantized.pth'\n",
        "\n",
        "train_batch_size = 30\n",
        "eval_batch_size = 50\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "float_model = load_model(saved_model_dir + float_model_file).to('cpu')\n",
        "\n",
        "\n",
        "\n",
        "float_model.eval()\n",
        "\n",
        "# Fuses modules\n",
        "float_model.fuse_model()\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsFr_2IzMFeg",
        "outputId": "d8b78b69-cb55-419d-ca0d-bccd837886da"
      },
      "source": [
        "num_eval_batches = 1000\n",
        "\n",
        "print(\"Size of baseline model\")\n",
        "print_size_of_model(float_model)\n",
        "\n",
        "top1, top5 = evaluate(float_model, criterion, dataloaders['val'], neval_batches=num_eval_batches)\n",
        "print('\\nEvaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg))\n",
        "torch.jit.save(torch.jit.script(float_model), saved_model_dir + scripted_float_model_file)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of baseline model\n",
            "Size (MB): 0.283271\n",
            "..\n",
            "Evaluation accuracy on 50000 images, 37.44\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvQy3dFoMI02",
        "outputId": "981ecd1a-b015-4864-ed16-c119cdf33cf3"
      },
      "source": [
        "num_calibration_batches = 32\n",
        "\n",
        "myModel = load_model(saved_model_dir + float_model_file).to('cpu')\n",
        "myModel.eval()\n",
        "\n",
        "# Fuse Conv, bn and relu\n",
        "myModel.fuse_model()\n",
        "\n",
        "# Specify quantization configuration\n",
        "# Start with simple min/max range estimation and per-tensor quantization of weights\n",
        "myModel.qconfig = torch.quantization.default_qconfig\n",
        "print(myModel.qconfig)\n",
        "backend = \"fbgemm\"\n",
        "\n",
        "torch.backends.quantized.engine = backend\n",
        "torch.quantization.prepare(myModel, inplace=True)\n",
        "\n",
        "# Calibrate first\n",
        "print('Post Training Quantization Prepare: Inserting Observers')\n",
        "#print('\\n Inverted Residual Block:After observer insertion \\n\\n', myModel.features[1].conv)\n",
        "\n",
        "# Calibrate with the training set\n",
        "evaluate(myModel, criterion, dataloaders['val'], neval_batches=num_calibration_batches)\n",
        "print('Post Training Quantization: Calibration done')\n",
        "\n",
        "# Convert to quantized model\n",
        "torch.quantization.convert(myModel, inplace=True)\n",
        "print('Post Training Quantization: Convert done')\n",
        "#print('\\n Inverted Residual Block: After fusion and quantization, note fused modules: \\n\\n',myModel.features[1].conv)\n",
        "\n",
        "print(\"Size of model after quantization\")\n",
        "print_size_of_model(myModel)\n",
        "\n",
        "top1, top5 = evaluate(myModel, criterion, dataloaders['val'], neval_batches=num_eval_batches)\n",
        "print('\\nEvaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, reduce_range=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric){})\n",
            "Post Training Quantization Prepare: Inserting Observers\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/ao/quantization/observer.py:174: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  reduce_range will be deprecated in a future release of PyTorch.\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "..Post Training Quantization: Calibration done\n",
            "Post Training Quantization: Convert done\n",
            "Size of model after quantization\n",
            "Size (MB): 0.076719\n",
            "..\n",
            "Evaluation accuracy on 50000 images, 37.62\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVLeK-uYMKfG",
        "outputId": "8b247bbf-361e-4981-f814-fee3a947d9f9"
      },
      "source": [
        "per_channel_quantized_model = load_model(saved_model_dir + float_model_file)\n",
        "per_channel_quantized_model.eval()\n",
        "per_channel_quantized_model.fuse_model()\n",
        "per_channel_quantized_model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
        "print(per_channel_quantized_model.qconfig)\n",
        "\n",
        "torch.quantization.prepare(per_channel_quantized_model, inplace=True)\n",
        "evaluate(per_channel_quantized_model,criterion, dataloaders['val'], num_calibration_batches)\n",
        "torch.quantization.convert(per_channel_quantized_model, inplace=True)\n",
        "top1, top5 = evaluate(per_channel_quantized_model, criterion, dataloaders['val'], neval_batches=num_eval_batches)\n",
        "print('\\nEvaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg))\n",
        "torch.jit.save(torch.jit.script(per_channel_quantized_model), saved_model_dir + scripted_quantized_model_file)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/ao/quantization/observer.py:174: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  reduce_range will be deprecated in a future release of PyTorch.\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/ao/quantization/observer.py:886: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  src_bin_begin // dst_bin_width, 0, self.dst_nbins - 1\n",
            "/usr/local/lib/python3.7/dist-packages/torch/ao/quantization/observer.py:891: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  src_bin_end // dst_bin_width, 0, self.dst_nbins - 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "..\n",
            "Evaluation accuracy on 50000 images, 37.66\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3xQMf6JMMWK"
      },
      "source": [
        "def train_one_epoch(model, criterion, optimizer, data_loader, device, ntrain_batches):\n",
        "    model.train()\n",
        "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
        "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
        "    avgloss = AverageMeter('Loss', '1.5f')\n",
        "\n",
        "    cnt = 0\n",
        "    for image, target in data_loader:\n",
        "        start_time = time.time()\n",
        "        print('.', end = '')\n",
        "        cnt += 1\n",
        "        image, target = image.to(device), target.to(device)\n",
        "        output = model(image)\n",
        "        loss = criterion(output, target)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
        "        top1.update(acc1[0], image.size(0))\n",
        "        top5.update(acc5[0], image.size(0))\n",
        "        avgloss.update(loss, image.size(0))\n",
        "        if cnt >= ntrain_batches:\n",
        "            print('Loss', avgloss.avg)\n",
        "\n",
        "            print('Training: * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'\n",
        "                  .format(top1=top1, top5=top5))\n",
        "            return\n",
        "\n",
        "    print('Full CIFAR10 train set:  * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'\n",
        "          .format(top1=top1, top5=top5))\n",
        "    return"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZku6BJjMObg"
      },
      "source": [
        "qat_model = load_model(saved_model_dir + float_model_file)\n",
        "qat_model.fuse_model()\n",
        "\n",
        "optimizer = torch.optim.SGD(qat_model.parameters(), lr = 0.0001)\n",
        "qat_model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeR3gzkAMRs3",
        "outputId": "2d7c0312-58ee-44c6-f69b-764c7392230f"
      },
      "source": [
        "torch.quantization.prepare_qat(qat_model, inplace=True)\n",
        "#print('Inverted Residual Block: After preparation for QAT, note fake-quantization modules \\n',qat_model.features[1].conv)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/ao/quantization/observer.py:174: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  reduce_range will be deprecated in a future release of PyTorch.\"\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNN(\n",
              "  (feats): Sequential(\n",
              "    (0): Conv2d(\n",
              "      3, 20, kernel_size=(3, 3), stride=(1, 1), bias=False\n",
              "      (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
              "        fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
              "        (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
              "      )\n",
              "      (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
              "        fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
              "        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "      )\n",
              "    )\n",
              "    (1): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): BatchNorm2d(\n",
              "      20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "      (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
              "        fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
              "        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "      )\n",
              "    )\n",
              "    (4): ConvBNReLU(\n",
              "      (0): ConvBnReLU2d(\n",
              "        20, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
              "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
              "          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
              "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
              "        )\n",
              "        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
              "          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
              "          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "        )\n",
              "      )\n",
              "      (1): Identity()\n",
              "      (2): Identity()\n",
              "    )\n",
              "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (6): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (fc): Sequential(\n",
              "    (0): Dropout(p=0.4, inplace=False)\n",
              "    (1): Linear(\n",
              "      in_features=2304, out_features=10, bias=True\n",
              "      (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
              "        fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
              "        (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
              "      )\n",
              "      (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
              "        fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
              "        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (quant): QuantStub(\n",
              "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
              "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
              "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "    )\n",
              "  )\n",
              "  (dequant): DeQuantStub()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W43dNZ_WMVWc",
        "outputId": "3ae7dbc0-21f2-4274-f42d-c110776bd7d1"
      },
      "source": [
        "def run_benchmark(model_file, img_loader):\n",
        "    elapsed = 0\n",
        "    model = torch.jit.load(model_file)\n",
        "    model.eval()\n",
        "    num_batches = 5\n",
        "    # Run the scripted model on a few batches of images\n",
        "    for i, (images, target) in enumerate(img_loader):\n",
        "        if i < num_batches:\n",
        "            start = time.time()\n",
        "            output = model(images)\n",
        "            end = time.time()\n",
        "            elapsed = elapsed + (end-start)\n",
        "        else:\n",
        "            break\n",
        "    num_images = images.size()[0] * num_batches\n",
        "\n",
        "    print('Elapsed time: %3.0f ms' % (elapsed/num_images*1000))\n",
        "    return elapsed\n",
        "\n",
        "run_benchmark(saved_model_dir + scripted_float_model_file, dataloaders['val'])\n",
        "\n",
        "run_benchmark(saved_model_dir + scripted_quantized_model_file, dataloaders['val'])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elapsed time:   1 ms\n",
            "Elapsed time:   0 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.743006706237793"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7uT-vu5MThN",
        "outputId": "74d98c70-bfe6-4253-ecc6-b9d7f026fbba"
      },
      "source": [
        "num_train_batches = 20\n",
        "\n",
        "# QAT takes time and one needs to train over a few epochs.\n",
        "# Train and check accuracy after each epoch\n",
        "for nepoch in range(8):\n",
        "    train_one_epoch(qat_model, criterion, optimizer, dataloaders['test'], torch.device('cpu'), num_train_batches)\n",
        "    if nepoch > 3:\n",
        "        # Freeze quantizer parameters\n",
        "        qat_model.apply(torch.quantization.disable_observer)\n",
        "    if nepoch > 2:\n",
        "        # Freeze batch norm mean and variance estimates\n",
        "        qat_model.apply(torch.nn.intrinsic.qat.freeze_bn_stats)\n",
        "\n",
        "    # Check the accuracy after each epoch\n",
        "    quantized_model = torch.quantization.convert(qat_model.eval(), inplace=False)\n",
        "    quantized_model.eval()\n",
        "    top1, top5 = evaluate(quantized_model,criterion, dataloaders['val'], neval_batches=num_eval_batches)\n",
        "    print('\\nEpoch %d :Evaluation accuracy on %d images, %2.2f'%(nepoch, num_eval_batches * eval_batch_size, top1.avg))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "..Full CIFAR10 train set:  * Acc@1 29.160 Acc@5 81.620\n",
            "..\n",
            "Epoch 0 :Evaluation accuracy on 50000 images, 37.78\n",
            "..Full CIFAR10 train set:  * Acc@1 30.700 Acc@5 80.760\n",
            "..\n",
            "Epoch 1 :Evaluation accuracy on 50000 images, 37.70\n",
            "..Full CIFAR10 train set:  * Acc@1 30.200 Acc@5 80.840\n",
            "..\n",
            "Epoch 2 :Evaluation accuracy on 50000 images, 37.72\n",
            "..Full CIFAR10 train set:  * Acc@1 29.460 Acc@5 81.660\n",
            "..\n",
            "Epoch 3 :Evaluation accuracy on 50000 images, 37.78\n",
            "..Full CIFAR10 train set:  * Acc@1 30.120 Acc@5 81.020\n",
            "..\n",
            "Epoch 4 :Evaluation accuracy on 50000 images, 37.84\n",
            "..Full CIFAR10 train set:  * Acc@1 29.320 Acc@5 80.640\n",
            "..\n",
            "Epoch 5 :Evaluation accuracy on 50000 images, 37.78\n",
            "..Full CIFAR10 train set:  * Acc@1 29.560 Acc@5 81.500\n",
            "..\n",
            "Epoch 6 :Evaluation accuracy on 50000 images, 37.74\n",
            "..Full CIFAR10 train set:  * Acc@1 29.240 Acc@5 81.760\n",
            "..\n",
            "Epoch 7 :Evaluation accuracy on 50000 images, 37.76\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBRK7tg7Kd9D"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}